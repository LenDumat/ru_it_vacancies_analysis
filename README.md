# Анализ рынка IT-вакансий РФ 
## Оглавление
 + [Зачем?](#why)
 + [Описание проекта](#description)
 + [Демо](#demo)
 + [Установка](#installation)
 + [Выводы](#conclusions)
 + [Данные](#data)
 
 ### <a name="why"></a> Зачем?
 В 2022 году стало происходить очень много интересных и непредвиденных событий, в следствие которых стало интересно посмотреть на числа и их тренды. Два главных вопроса, на которые хотелось бы найти ответ:
 1. Насколько всё плохо?
 2. Куда всё движется?
 
 ### <a name="description"></a> Описание проекта
 #### Постановка задачи 
 ~~На подножном корму~~ Из открытых источников достать всю доступную информацию по вакансиям в IT-сфере в РФ. Из полученных данных построить аналитический дашборд, который бы отражал общее состояние рынка.
 
 #### Что получилось
 1. 5 парсеров, которые вытягивают данные в csv формате со следующих сайтов:
    * [HeadHunter](https://hh.ru/) - тут удалось отделаться официальным [API](https://github.com/hhru/api)
    * [HabrCareer](https://career.habr.com/) - пришлось парсить
    * [GeekJob](https://geekjob.ru/) - пришлось парсить
    * [GetMatch](https://getmatch.ru/) - пришлось парсить
    * [Сайт ЦБ РФ](http://www.cbr.ru/) - отсюда беру курсы валют по дням через [API](http://www.cbr.ru/development/sxml/)
 2. БД sqlite, в которую заливаю данные из csv файлов
 3. [Dash](https://dash.plotly.com/) аппликуха, которая берёт данные из БД и строит графики
 
 #### ~~Красивая~~ картинка
 ![Vacancies Parser](https://user-images.githubusercontent.com/35892153/215205911-fb030c1a-24ca-48f3-810e-649aafab3169.jpg)

 ### <a name="demo"></a> Демо
 Можно потыкать в дашбоард [тут](http://46.29.163.180:8050/).
 Если что-то отвалилось ~~или я забил на хостинг~~, то вот гифка как это должно работать:
 ![Анимация](https://user-images.githubusercontent.com/35892153/215260179-dd339550-80b8-47f1-b7be-5b3f795e6c2e.gif)

 
 ### <a name="installation"></a> Установка
 Тут инструкция как сделал я на своей linux машине. Конечно, есть [более простые способы](https://dash.plotly.com/deployment) с использованием хостингов, но так неинтересно.
 Что понадобится:
 1. Linux машина с хотя бы 1 Гб оперативной памяти, хоть каким-нибудь процессором и >10 Гб постоянной памяти. Я использовал Ubuntu 20.
 2. Список прокси - хотя бы одна прокся, но чем больше, тем выше скорость парсинга.
 
 Шаги по установке:
 1. Клонируем репо
 2. Создаем виртуальное окружение
   ```
   pip install virtualenv
   virtualenv vacancies_venv 
   source vacancies_venv/bin/activate
   ```
 2. Ставим все пакеты из requirements.txt
   ```
   pip install -r requirements.txt
   ```
 3. Вставляем в proxies.txt рабочие прокси. Формат внутри файла описан.
 4. Запускаем единоразово create_cron.py - создадутся cron джобы для запуска парсеров по расписанию.
   ```
   python3 create_cron.py
   ```
 5. Запускам единоразово create_db.py - создастся БД со всеми объектами.
   ```
   python3 create_db.py
   ```
 6. Поднимаем gunicorn командой
   ```
   gunicorn dash_app:server -b :8050
   ```
 7. ~~Ставим лайки, подписываемся на канал~~

 ### <a name="conclusions"></a> Выводы
 Ответить вопросы, поставленные в секции [Зачем?](#why) очевидно не получилось. Для того, чтобы понимать что и куда движется нужно иметь данные не за последние пару месяцев, а хотя бы за последние полгода. Поэтому, парсеры проработают пока мне не надоест, может что-то и удастся понять.
 
 Зато я точно понял как делать не надо:
 1. Плохая идея перекладывать калькуляцию каких то параметров на сам дашбоард - лучше чтобы в базе лежали уже готовые данные, а дашбоард их будет только показывать.
 2. Cron - очевидно не лучший шедулер. Если нет ограничений по мощности хоста, то лучше воспользоваться хотя бы [Luigi](https://luigi.readthedocs.io/en/stable/)
 3. Лучше планировать всю структуру проекта до того как всё написал
 ![image](https://user-images.githubusercontent.com/35892153/215257265-5f29b4de-2053-4489-9006-fbff4cdd9569.png)
